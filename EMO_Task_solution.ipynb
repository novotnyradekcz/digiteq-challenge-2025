{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiling emoji detected at: (437, 284)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smiling emoji detected at: (361, 82)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m window \u001b[38;5;241m=\u001b[39m image_array[y:y\u001b[38;5;241m+\u001b[39mwindow_size, x:x\u001b[38;5;241m+\u001b[39mwindow_size]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Compare with template\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mmse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# If MSE is low, it's a match\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;241m<\u001b[39m threshold:\n",
      "Cell \u001b[0;32mIn[99], line 7\u001b[0m, in \u001b[0;36mmse\u001b[0;34m(imageA, imageB)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmse\u001b[39m(imageA, imageB):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate Mean Squared Error between two images.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     err \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimageA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimageB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     err \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(imageA\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m imageA\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m err\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/_core/fromnumeric.py:2389\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2386\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[1;32m   2392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/numpy/_core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mse(imageA, imageB):\n",
    "    \"\"\"Calculate Mean Squared Error between two images.\"\"\"\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    return err\n",
    "\n",
    "for i in range(101):\n",
    "    # Load images using PIL\n",
    "    try:\n",
    "        image = Image.open(f\"data/basic/dataset/emoji_{i}.jpg\").convert(\"L\")  # Convert to grayscale\n",
    "    except:\n",
    "        continue\n",
    "    template = Image.open(\"data/basic/dataset/emoji_to_find.jpg\").convert(\"L\")\n",
    "\n",
    "    # Convert images to NumPy arrays\n",
    "    image_array = np.array(image)\n",
    "    template_array = np.array(template)\n",
    "\n",
    "    # Convert grayscale image to RGB for drawing\n",
    "    image_rgb = image.convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image_rgb)  # Enable drawing\n",
    "\n",
    "    # Sliding window parameters\n",
    "    window_size = 50\n",
    "    step_size = 1\n",
    "    threshold = 500  # Adjust this for matching accuracy\n",
    "\n",
    "    match = None\n",
    "\n",
    "    # Slide over the image\n",
    "    for y in range(0, image_array.shape[0] - window_size, step_size):\n",
    "        for x in range(0, image_array.shape[1] - window_size, step_size):\n",
    "            # Extract window\n",
    "            window = image_array[y:y+window_size, x:x+window_size]\n",
    "\n",
    "            # Compare with template\n",
    "            error = mse(window, template_array)\n",
    "        \n",
    "            # If MSE is low, it's a match\n",
    "            if error < threshold:\n",
    "                match = (x, y)\n",
    "                draw.rectangle([x, y, x+window_size, y+window_size], outline=\"green\", width=2)\n",
    "\n",
    "    # Display the image with detected matches\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_rgb)  # Ensure correct color interpretation\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print detected positions\n",
    "    if match is not None:\n",
    "        print(f\"Smiling emoji detected at: {match}\")\n",
    "    else:\n",
    "        print(\"Smiling emoji not found.\")\n",
    "    match = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji: happy Matches: 1\n",
      "Emoji: sad Matches: 2\n",
      "Emoji: crying Matches: 11\n",
      "Emoji: surprised Matches: 2\n",
      "Emoji: angry Matches: 3\n",
      "Detected 1 emojis.\n",
      "Picture: data/train/dataset/emoji_1.jpg Emoji: crying Coordinates: (475, 263)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Folder containing emoji templates (each emoji type is a separate 50x50 jpg file)\n",
    "emoji_types = [\"happy\", \"sad\", \"crying\", \"surprised\", \"angry\"]\n",
    "emoji_templates = {emoji: cv2.imread(f\"data/emojis/{emoji}.jpg\", cv2.IMREAD_GRAYSCALE) for emoji in emoji_types}\n",
    "# print(emoji_templates)\n",
    "\n",
    "# Load the input image\n",
    "image_name = \"data/train/dataset/emoji_1.jpg\"  # Change to the correct filename\n",
    "image = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create SIFT detector (or use ORB if OpenCV does not support SIFT)\n",
    "sift = cv2.SIFT_create(nfeatures=1000)  # Use cv2.ORB_create() if SIFT is unavailable\n",
    "# sift = cv2.ORB_create(nfeatures=100)\n",
    "\n",
    "# Detect keypoints and descriptors for the input image\n",
    "keypoints_img, descriptors_img = sift.detectAndCompute(image, None)\n",
    "\n",
    "# Store detected emoji results\n",
    "detected_emojis = []\n",
    "\n",
    "# Match each emoji template\n",
    "for emoji_name, template in emoji_templates.items():\n",
    "    if template is None:\n",
    "        print(f\"Error loading template: {emoji_name}.jpg\")\n",
    "        continue  # Skip if template is missing\n",
    "\n",
    "    # Detect keypoints and descriptors for the emoji template\n",
    "    keypoints_tmpl, descriptors_tmpl = sift.detectAndCompute(template, None)\n",
    "\n",
    "    if descriptors_tmpl is None:\n",
    "        print(f\"Skipping {emoji_name}: No descriptors found.\")\n",
    "        continue\n",
    "\n",
    "    # Use FLANN-based matcher for SIFT\n",
    "    index_params = dict(algorithm=1, trees=10)  # KDTree for SIFT\n",
    "    search_params = dict(checks=100)\n",
    "    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    # Find matches\n",
    "    matches = matcher.knnMatch(descriptors_tmpl, descriptors_img, k=2)\n",
    "\n",
    "    # Apply Lowe’s ratio test to filter good matches\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.9 * n.distance:  # Threshold for good matches\n",
    "            good_matches.append(m)\n",
    "\n",
    "    print(f\"Emoji: {emoji_name} Matches: {len(good_matches)}\")\n",
    "\n",
    "    # If we have enough good matches, estimate emoji position\n",
    "    if len(good_matches) > 3:  # Require at least 4 good matches (4 points needed for findHomography)\n",
    "        src_pts = np.float32([keypoints_tmpl[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints_img[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Find transformation matrix using RANSAC\n",
    "        # M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 10.0)\n",
    "        \n",
    "        # print(f\"M: {M}\")\n",
    "\n",
    "        # if M is not None:\n",
    "        #     # Get the center of the matched emoji\n",
    "        #     mean_x = int(np.mean(dst_pts[:, 0, 0]))\n",
    "        #     mean_y = int(np.mean(dst_pts[:, 0, 1]))\n",
    "\n",
    "        #     detected_emojis.append((emoji_name, (mean_x, mean_y)))\n",
    "        \n",
    "        # Get the corner of the matched emoji\n",
    "        mean_x = int(np.min(dst_pts[:, 0, 0]))\n",
    "        mean_y = int(np.min(dst_pts[:, 0, 1]))\n",
    "\n",
    "        detected_emojis.append((emoji_name, (mean_x, mean_y)))\n",
    "\n",
    "print(f\"Detected {len(detected_emojis)} emojis.\")\n",
    "\n",
    "# Print results\n",
    "for emoji_name, (x, y) in detected_emojis:\n",
    "    print(f\"Picture: {image_name} Emoji: {emoji_name} Coordinates: ({x}, {y})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji: happy Matches: 0\n",
      "Emoji: sad Matches: 2\n",
      "Emoji: crying Matches: 2\n",
      "Emoji: surprised Matches: 0\n",
      "Emoji: angry Matches: 9\n",
      "Detected 2 emojis.\n",
      "Picture: data/train/dataset/emoji_813.jpg Emoji: happy Coordinates: (724, 205)\n",
      "Picture: data/train/dataset/emoji_813.jpg Emoji: happy Coordinates: (743, 180)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load emoji templates (each emoji type is a 50x50 jpg file)\n",
    "emoji_types = [\"happy\", \"sad\", \"crying\", \"surprised\", \"angry\"]\n",
    "emoji_templates = {emoji: cv2.imread(f\"data/emojis/{emoji}.jpg\", cv2.IMREAD_GRAYSCALE) for emoji in emoji_types}\n",
    "\n",
    "# Load the input image\n",
    "image_name = \"data/train/dataset/emoji_813.jpg\"  # Change to the correct filename\n",
    "image = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create SIFT detector (or ORB if SIFT is unavailable)\n",
    "sift = cv2.SIFT_create(nfeatures=1000)\n",
    "\n",
    "# Detect keypoints and descriptors for the input image\n",
    "keypoints_img, descriptors_img = sift.detectAndCompute(image, None)\n",
    "\n",
    "# Store detected emoji results\n",
    "detected_regions = []  # Stores locations of all detected emoji keypoints\n",
    "\n",
    "# Step 1: Detect possible emoji locations first (without classification)\n",
    "for emoji_name, template in emoji_templates.items():\n",
    "    if template is None:\n",
    "        print(f\"Error loading template: {emoji_name}.jpg\")\n",
    "        continue\n",
    "\n",
    "    keypoints_tmpl, descriptors_tmpl = sift.detectAndCompute(template, None)\n",
    "\n",
    "    if descriptors_tmpl is None:\n",
    "        print(f\"Skipping {emoji_name}: No descriptors found.\")\n",
    "        continue\n",
    "\n",
    "    # Use FLANN-based matcher\n",
    "    index_params = dict(algorithm=1, trees=10)  # KDTree for SIFT\n",
    "    search_params = dict(checks=100)\n",
    "    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    # Find matches\n",
    "    matches = matcher.knnMatch(descriptors_tmpl, descriptors_img, k=2)\n",
    "\n",
    "    # Apply Lowe’s ratio test to filter good matches\n",
    "    good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]\n",
    "\n",
    "    print(f\"Emoji: {emoji_name} Matches: {len(good_matches)}\")\n",
    "\n",
    "    # If we have enough good matches, estimate emoji position\n",
    "    if len(good_matches) > 3:\n",
    "        dst_pts = np.float32([keypoints_img[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        for pt in dst_pts:\n",
    "            x, y = int(pt[0][0]), int(pt[0][1])\n",
    "            detected_regions.append((x, y))  # Store the location for classification\n",
    "\n",
    "# Step 2: Cluster nearby keypoints to avoid duplicate detections\n",
    "def cluster_keypoints(detected_regions, threshold=30):\n",
    "    \"\"\"\n",
    "    Groups keypoints that are close together into clusters.\n",
    "    \"\"\"\n",
    "    clusters = []\n",
    "    for x, y in detected_regions:\n",
    "        found_cluster = False\n",
    "        for cluster in clusters:\n",
    "            if np.linalg.norm(np.array(cluster) - np.array((x, y))) < threshold:\n",
    "                found_cluster = True\n",
    "                break\n",
    "        if not found_cluster:\n",
    "            clusters.append((x, y))\n",
    "    return clusters\n",
    "\n",
    "filtered_regions = cluster_keypoints(detected_regions)\n",
    "\n",
    "# Step 3: Classify each detected region\n",
    "detected_emojis = []\n",
    "\n",
    "for (x, y) in filtered_regions:\n",
    "    # Extract a 50x50 region around the detected point\n",
    "    x1, y1 = max(0, x - 25), max(0, y - 25)\n",
    "    x2, y2 = min(image.shape[1], x + 25), min(image.shape[0], y + 25)\n",
    "    detected_emoji_region = image[y1:y2, x1:x2]\n",
    "\n",
    "    # Compare with emoji templates using histogram similarity\n",
    "    best_match = None\n",
    "    best_score = -1\n",
    "\n",
    "    for emoji_name, template in emoji_templates.items():\n",
    "        resized_template = cv2.resize(template, (x2 - x1, y2 - y1))  # Resize to match detected region\n",
    "\n",
    "        hist1 = cv2.calcHist([resized_template], [0], None, [256], [0, 256])\n",
    "        hist2 = cv2.calcHist([detected_emoji_region], [0], None, [256], [0, 256])\n",
    "\n",
    "        score = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = emoji_name\n",
    "\n",
    "    detected_emojis.append((best_match, (x, y)))\n",
    "\n",
    "print(f\"Detected {len(detected_emojis)} emojis.\")\n",
    "\n",
    "# Print results\n",
    "for emoji_name, (x, y) in detected_emojis:\n",
    "    print(f\"Picture: {image_name} Emoji: {emoji_name} Coordinates: ({x}, {y})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji: happy Matches: 0\n",
      "Emoji: sad Matches: 0\n",
      "Emoji: crying Matches: 2\n",
      "Emoji: surprised Matches: 0\n",
      "Emoji: angry Matches: 3\n",
      "Detected 0 emojis.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load emoji templates (each emoji type is a separate 50x50 jpg file)\n",
    "emoji_types = [\"happy\", \"sad\", \"crying\", \"surprised\", \"angry\"]\n",
    "emoji_templates = {emoji: cv2.imread(f\"data/emojis/{emoji}.jpg\", cv2.IMREAD_GRAYSCALE) for emoji in emoji_types}\n",
    "\n",
    "# Load the input image\n",
    "image_name = \"data/train/dataset/emoji_1115.jpg\"  # Change to the correct filename\n",
    "image = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create SIFT detector\n",
    "sift = cv2.SIFT_create(nfeatures=1000)\n",
    "\n",
    "# Detect keypoints and descriptors for the input image\n",
    "keypoints_img, descriptors_img = sift.detectAndCompute(image, None)\n",
    "\n",
    "# Store detected emoji results\n",
    "detected_regions = []\n",
    "\n",
    "# Step 1: Detect potential emoji locations first (without classification)\n",
    "for emoji_name, template in emoji_templates.items():\n",
    "    if template is None:\n",
    "        print(f\"Error loading template: {emoji_name}.jpg\")\n",
    "        continue\n",
    "\n",
    "    # Detect keypoints and descriptors for the emoji template\n",
    "    keypoints_tmpl, descriptors_tmpl = sift.detectAndCompute(template, None)\n",
    "    \n",
    "    if descriptors_tmpl is None:\n",
    "        print(f\"Skipping {emoji_name}: No descriptors found.\")\n",
    "        continue\n",
    "\n",
    "    # Use FLANN-based matcher\n",
    "    index_params = dict(algorithm=1, trees=10)  # KDTree for SIFT\n",
    "    search_params = dict(checks=100)\n",
    "    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    # Find matches\n",
    "    matches = matcher.knnMatch(descriptors_tmpl, descriptors_img, k=2)\n",
    "\n",
    "    # Apply Lowe’s ratio test to filter good matches\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:  # Threshold for good matches\n",
    "            good_matches.append(m)\n",
    "\n",
    "    print(f\"Emoji: {emoji_name} Matches: {len(good_matches)}\")\n",
    "\n",
    "    # If we have enough good matches, estimate emoji position\n",
    "    if len(good_matches) > 3:  # Require at least 3 good matches (4 points needed for findHomography)\n",
    "        src_pts = np.float32([keypoints_tmpl[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints_img[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Use RANSAC to estimate the transformation matrix (homography)\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        if M is not None:\n",
    "            # Get transformed corners of the matched emoji\n",
    "            h, w = template.shape\n",
    "            corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n",
    "            transformed_corners = cv2.perspectiveTransform(corners, M)\n",
    "\n",
    "            # Get the bounding box for the emoji\n",
    "            min_x = int(min(transformed_corners[:, 0, 0]))\n",
    "            max_x = int(max(transformed_corners[:, 0, 0]))\n",
    "            min_y = int(min(transformed_corners[:, 0, 1]))\n",
    "            max_y = int(max(transformed_corners[:, 0, 1]))\n",
    "\n",
    "            detected_regions.append((emoji_name, min_x, min_y, max_x, max_y))\n",
    "\n",
    "# Step 2: Filter overlapping detections (cluster nearby detections)\n",
    "def filter_overlapping_regions(detected_regions, min_distance=30):\n",
    "    filtered = []\n",
    "    for emoji_name, x1, y1, x2, y2 in detected_regions:\n",
    "        if not any(\n",
    "            abs(x1 - fx1) < min_distance and abs(y1 - fy1) < min_distance\n",
    "            for _, fx1, fy1, _, _ in filtered\n",
    "        ):\n",
    "            filtered.append((emoji_name, x1, y1, x2, y2))\n",
    "    return filtered\n",
    "\n",
    "filtered_emojis = filter_overlapping_regions(detected_regions)\n",
    "\n",
    "# Step 3: Print results\n",
    "print(f\"Detected {len(filtered_emojis)} emojis.\")\n",
    "for emoji_name, x1, y1, x2, y2 in filtered_emojis:\n",
    "    print(f\"Picture: {image_name} Emoji: {emoji_name} Coordinates: ({x1}, {y1})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/flann/src/miniflann.cpp:521: error: (-215:Assertion failed) (size_t)knn <= index_->size() in function 'runKnnSearch_'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m matcher \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mFlannBasedMatcher(index_params, search_params)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Find matches\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknnMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescriptors_tmpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptors_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Apply Lowe’s ratio test to filter good matches\u001b[39;00m\n\u001b[1;32m     43\u001b[0m good_matches \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/flann/src/miniflann.cpp:521: error: (-215:Assertion failed) (size_t)knn <= index_->size() in function 'runKnnSearch_'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load emoji templates (each emoji type is a separate 50x50 jpg file)\n",
    "emoji_types = [\"happy\", \"sad\", \"crying\", \"surprised\", \"angry\"]\n",
    "emoji_templates = {emoji: cv2.imread(f\"data/emojis/{emoji}.jpg\", cv2.IMREAD_GRAYSCALE) for emoji in emoji_types}\n",
    "\n",
    "# Load the input image\n",
    "image_name = \"data/train/dataset/emoji_474.jpg\"  # Change to the correct filename\n",
    "image = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create SIFT detector\n",
    "sift = cv2.SIFT_create(nfeatures=1000)\n",
    "\n",
    "# Detect keypoints and descriptors for the input image\n",
    "keypoints_img, descriptors_img = sift.detectAndCompute(image, None)\n",
    "\n",
    "# Store detected emoji results\n",
    "detected_regions = []\n",
    "\n",
    "# Step 1: Detect potential emoji locations first (without classification)\n",
    "for emoji_name, template in emoji_templates.items():\n",
    "    if template is None:\n",
    "        print(f\"Error loading template: {emoji_name}.jpg\")\n",
    "        continue\n",
    "\n",
    "    # Detect keypoints and descriptors for the emoji template\n",
    "    keypoints_tmpl, descriptors_tmpl = sift.detectAndCompute(template, None)\n",
    "    \n",
    "    if descriptors_tmpl is None:\n",
    "        print(f\"Skipping {emoji_name}: No descriptors found.\")\n",
    "        continue\n",
    "\n",
    "    # Use FLANN-based matcher\n",
    "    index_params = dict(algorithm=1, trees=10)  # KDTree for SIFT\n",
    "    search_params = dict(checks=100)\n",
    "    matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    # Find matches\n",
    "    matches = matcher.knnMatch(descriptors_tmpl, descriptors_img, k=2)\n",
    "\n",
    "    # Apply Lowe’s ratio test to filter good matches\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.9 * n.distance:  # Threshold for good matches (adjust for more robustness)\n",
    "            good_matches.append(m)\n",
    "\n",
    "    # print(f\"Emoji: {emoji_name} Matches: {len(good_matches)}\")\n",
    "\n",
    "    # If we have enough good matches, proceed\n",
    "    if len(good_matches) > 5:  # Require at least 5 good matches for better accuracy\n",
    "        src_pts = np.float32([keypoints_tmpl[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([keypoints_img[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        # Step 2: Use Homography (RANSAC) to find the transformation matrix\n",
    "        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)  # 5.0 is the threshold for outlier rejection\n",
    "\n",
    "        if M is not None:\n",
    "            # Step 3: Apply the transformation to the template's corners to get the transformed bounding box\n",
    "            h, w = template.shape\n",
    "            corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n",
    "            transformed_corners = cv2.perspectiveTransform(corners, M)\n",
    "\n",
    "            # Step 4: Get the bounding box of the transformed corners\n",
    "            min_x = int(min(transformed_corners[:, 0, 0]))\n",
    "            max_x = int(max(transformed_corners[:, 0, 0]))\n",
    "            min_y = int(min(transformed_corners[:, 0, 1]))\n",
    "            max_y = int(max(transformed_corners[:, 0, 1]))\n",
    "\n",
    "            # Add the detected region to the list\n",
    "            detected_regions.append((emoji_name, min_x, min_y, max_x, max_y))\n",
    "\n",
    "# Step 5: Filter overlapping detections (cluster nearby detections)\n",
    "def filter_overlapping_regions(detected_regions, min_distance=30):\n",
    "    filtered = []\n",
    "    for emoji_name, x1, y1, x2, y2 in detected_regions:\n",
    "        if not any(\n",
    "            abs(x1 - fx1) < min_distance and abs(y1 - fy1) < min_distance\n",
    "            for _, fx1, fy1, _, _ in filtered\n",
    "        ):\n",
    "            filtered.append((emoji_name, x1, y1, x2, y2))\n",
    "    return filtered\n",
    "\n",
    "filtered_emojis = filter_overlapping_regions(detected_regions)\n",
    "\n",
    "# Step 6: Print results\n",
    "# print(f\"Detected {len(filtered_emojis)} emojis.\")\n",
    "print(f\"Picture: {image_name}\")\n",
    "for emoji_name, x1, y1, x2, y2 in filtered_emojis:\n",
    "    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    print(f\"Emoji: {emoji_name} Coordinates: ({x1}, {y1})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picture: emoji_0.jpg\n",
      "Emoji: angry Coordinates: (436, 371)\n",
      "Emoji: sad Coordinates: (605, 304)\n",
      "Emoji: happy Coordinates: (490, 281)\n",
      "Picture: emoji_1.jpg\n",
      "Emoji: angry Coordinates: (462, 256)\n",
      "Picture: emoji_2.jpg\n",
      "Emoji: angry Coordinates: (231, 339)\n",
      "Picture: emoji_3.jpg\n",
      "Emoji: happy Coordinates: (136, 124)\n",
      "Picture: emoji_4.jpg\n",
      "Emoji: sad Coordinates: (704, 287)\n",
      "Picture: emoji_5.jpg\n",
      "Emoji: happy Coordinates: (609, 331)\n",
      "Picture: emoji_6.jpg\n",
      "Emoji: happy Coordinates: (441, 497)\n",
      "Picture: emoji_7.jpg\n",
      "Emoji: surprised Coordinates: (260, 394)\n",
      "Picture: emoji_8.jpg\n",
      "Emoji: crying Coordinates: (225, 342)\n",
      "Picture: emoji_9.jpg\n",
      "Emoji: angry Coordinates: (531, 284)\n",
      "Picture: emoji_10.jpg\n",
      "Emoji: angry Coordinates: (378, 87)\n",
      "Picture: emoji_11.jpg\n",
      "Emoji: happy Coordinates: (541, 88)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def implementation_main():\n",
    "    # Load emoji templates (each emoji type is a separate 50x50 jpg file)\n",
    "    emoji_types = [\"happy\", \"sad\", \"crying\", \"surprised\", \"angry\"]\n",
    "    emoji_templates = {emoji: cv2.imread(f\"data/emojis/{emoji}.jpg\", cv2.IMREAD_GRAYSCALE) for emoji in emoji_types}\n",
    "    \n",
    "    # Create SIFT detector\n",
    "    sift = cv2.SIFT_create(nfeatures=1000)\n",
    "    \n",
    "    for i in range(12):\n",
    "        # Load the input image\n",
    "        image_name = f\"data/train/dataset/emoji_{i}.jpg\"  # Change to the correct filename\n",
    "        image = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Step 1: Thresholding or edge detection to find potential emoji areas\n",
    "        _, thresholded = cv2.threshold(image, 200, 255, cv2.THRESH_BINARY_INV)  # Inverse thresholding\n",
    "        # Alternatively, you can use Canny edge detection\n",
    "        # thresholded = cv2.Canny(image, 100, 200)\n",
    "\n",
    "        # Step 2: Find contours (regions of interest that might contain emojis)\n",
    "        contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # print(f\"Found {len(contours)} contours in {image_name}: {contours}\")\n",
    "\n",
    "        detected_regions = []\n",
    "        for contour in contours:\n",
    "            # Step 3: Filter small contours (that might not be emojis)\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area < 100:  # Minimum area threshold for emoji region\n",
    "                continue\n",
    "\n",
    "            # Get the bounding box around the contour\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "            # Extract the potential emoji region from the image\n",
    "            roi = image[y:y+h, x:x+w]\n",
    "            \n",
    "            # Step 4: Compare the detected region with emoji templates using SIFT\n",
    "            best_match = None\n",
    "            best_match_score = float('inf')  # We want to minimize the distance\n",
    "            \n",
    "            for emoji_name, template in emoji_templates.items():\n",
    "                if template is None:\n",
    "                    continue\n",
    "                \n",
    "                # Resize template to match the size of the detected region (for better matching)\n",
    "                resized_template = cv2.resize(template, (w, h))\n",
    "                \n",
    "                # Detect keypoints and descriptors for the detected region and template\n",
    "                keypoints_img, descriptors_img = sift.detectAndCompute(roi, None)\n",
    "                keypoints_tmpl, descriptors_tmpl = sift.detectAndCompute(resized_template, None)\n",
    "                \n",
    "                if descriptors_img is None or descriptors_tmpl is None:\n",
    "                    continue\n",
    "                \n",
    "                # Use FLANN-based matcher\n",
    "                index_params = dict(algorithm=1, trees=10)\n",
    "                search_params = dict(checks=100)\n",
    "                matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "                \n",
    "                # Find matches\n",
    "                matches = matcher.knnMatch(descriptors_tmpl, descriptors_img, k=2)\n",
    "                \n",
    "                # Apply Lowe’s ratio test\n",
    "                good_matches = [m for m, n in matches if m.distance < 0.9 * n.distance]\n",
    "                \n",
    "                # If good matches are found, compute the match score\n",
    "                if len(good_matches) > 5:  # Require a minimum number of good matches\n",
    "                    match_score = len(good_matches)\n",
    "                    if match_score < best_match_score:\n",
    "                        best_match_score = match_score\n",
    "                        best_match = emoji_name\n",
    "            \n",
    "            if best_match is not None:\n",
    "                # Add the detected region and the matched emoji to the list\n",
    "                detected_regions.append((best_match, x, y, x+w, y+h))\n",
    "\n",
    "        # Step 5: Filter overlapping detections (optional)\n",
    "        def filter_overlapping_regions(detected_regions, min_distance=30):\n",
    "            filtered = []\n",
    "            for emoji_name, x1, y1, x2, y2 in detected_regions:\n",
    "                if not any(\n",
    "                    abs(x1 - fx1) < min_distance and abs(y1 - fy1) < min_distance\n",
    "                    for _, fx1, fy1, _, _ in filtered\n",
    "                ):\n",
    "                    filtered.append((emoji_name, x1, y1, x2, y2))\n",
    "            return filtered\n",
    "        \n",
    "        filtered_emojis = filter_overlapping_regions(detected_regions)\n",
    "\n",
    "        # Step 6: Print results\n",
    "        print(f\"Picture: {image_name.split('/')[-1]}\")\n",
    "        for emoji_name, x1, y1, x2, y2 in filtered_emojis:\n",
    "            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            print(f\"Emoji: {emoji_name} Coordinates: ({x1}, {y1})\")\n",
    "\n",
    "implementation_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
